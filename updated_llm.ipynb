{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KicW7NkLPlyL"
   },
   "source": [
    "This code takes in a claim as a string an first parses it into multiple sentences then for each sentence it asynctronously calls into three datasets to check if the claims are true or false. Then it returns a result\n",
    "\n",
    "TODO:\n",
    "\n",
    "Give other ratings besides true or false for a claim(s)\n",
    "\n",
    "return a link to an article(s) with the true statement(s) // Sally\n",
    "\n",
    "integrate user feedbck // Sally\n",
    "\n",
    "Ensure url-safe input. example change wouldn't to would not. change % and $ to words etc.\n",
    "\n",
    "testing/benchmarks // Daniel\n",
    "\n",
    "process multiple claims // Sally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUl68-VbONoM"
   },
   "source": [
    "https://github.com/ParthaPRay/Ollama_GoogleColab_colabxterm_langchain/blob/main/Ollama_on_Colab_using_xterm_and_langchain.ipynb\n",
    "\n",
    "\n",
    "https://stackoverflow.com/questions/8551230/how-can-i-get-information-from-an-a-href-tag-within-div-tags-with-beautifuls\n",
    "\n",
    "https://www.mirascope.com/post/langchain-prompt-template\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OLLAMA_MODEL = \"llama3.2\"\n",
    "# OLLAMA_MODEL = \"llama2\"\n",
    "\n",
    "OLLAMA_HOST = \"http://host.docker.internal:11434\" # for when running within docker image\n",
    "# OLLAMA_HOST = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "curl is already the newest version (7.81.0-1ubuntu1.18).\n",
      "unzip is already the newest version (6.0-26ubuntu3.2).\n",
      "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libxcomposite-dev is already the newest version (1:0.4.5-1build2).\n",
      "libxdamage-dev is already the newest version (1:1.1.5-2build2).\n",
      "libxrandr-dev is already the newest version (2:1.5.2-1build1).\n",
      "libgbm-dev is already the newest version (23.2.1-1ubuntu3.1~22.04.2).\n",
      "libgtk-3-dev is already the newest version (3.24.33-1ubuntu2.2).\n",
      "libx11-dev is already the newest version (2:1.7.5-1ubuntu0.3).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt -q\n",
    "\n",
    "# !pip install playwright\n",
    "!apt-get update -y\n",
    "!apt-get install -y wget curl unzip\n",
    "!apt-get install -y libx11-dev libxcomposite-dev libxdamage-dev libxrandr-dev libgbm-dev libgtk-3-dev\n",
    "!playwright install\n",
    "!playwright install chromium\n",
    "\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Ollama\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYM8t-UcBoWm",
    "outputId": "d6d85040-834b-44e2-daa9-841d1a0a6514",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4529/3463873494.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_model = Ollama(model=OLLAMA_MODEL, base_url=OLLAMA_HOST)\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "ollama_model = Ollama(model=OLLAMA_MODEL, base_url=OLLAMA_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here\\'s an example of how you can implement the factorial function in C:\\n\\n```c\\n#include <stdio.h>\\n\\n// Function to calculate factorial\\nlong long calculateFactorial(int n) {\\n    if (n == 0 || n == 1)\\n        return 1;\\n    \\n    else \\n        return n * calculateFactorial(n-1);\\n}\\n\\nint main() {\\n    int num;\\n\\n    printf(\"Enter a number: \");\\n    scanf(\"%d\", &num);\\n\\n    if (num < 0) {\\n        printf(\"Sorry, factorial of negative numbers is not defined.\\\\n\");\\n    } else {\\n        long long result = calculateFactorial(num);\\n        printf(\"The factorial of %d is %lld\\\\n\", num, result);\\n    }\\n\\n    return 0;\\n}\\n```\\n\\nThis code calculates the factorial of a given number. It does this by recursively calling itself until it reaches the base case (i.e., when `n` equals 1), at which point it starts returning values back up the call stack.\\n\\nPlease note that the size of an integer data type is limited, and can cause issues if you try to calculate large factorials. In such cases, you might need to use a library like `stdlib.h` or a larger data type like `long long`.\\n\\nAlso note that this implementation uses recursion which may lead to a stack overflow for very large inputs.\\n\\nIn case of iterative approach for calculating factorial:\\n\\n```c\\n#include <stdio.h>\\n\\n// Function to calculate factorial using iteration\\nlong long calculateFactorial(int n) {\\n    if (n == 0 || n == 1)\\n        return 1;\\n    \\n    else \\n        return n * calculateFactorial(n-1);\\n}\\n\\nint main() {\\n    int num;\\n\\n    printf(\"Enter a number: \");\\n    scanf(\"%d\", &num);\\n\\n    if (num < 0) {\\n        printf(\"Sorry, factorial of negative numbers is not defined.\\\\n\");\\n    } else {\\n        long long result = calculateFactorial(num);\\n        printf(\"The factorial of %d is %lld\\\\n\", num, result);\\n    }\\n\\n    return 0;\\n}\\n```\\n\\nThis will use a loop to iterate from `1` to `n`, which makes it more efficient than the recursive approach.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_model.invoke(\"Write a factorial in c code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYM8t-UcBoWm",
    "outputId": "d6d85040-834b-44e2-daa9-841d1a0a6514",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a claim to be fact-checked:  We would not have left $85 billion worth of brand-new, beautiful military equipment behind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.factcheck.org/search/#gsc.tab=0&gsc.q=We%20would%20not%20have%20left%20$85%20billion%20worth%20of%20brand-new,%20beautiful%20military%20equipment%20behind&gsc.sort=\n",
      "FactCheck Articles: [{'title': 'Republicans Inflate Cost of Taliban-Seized U.S. Military Equipment', 'url': 'https://www.factcheck.org/2021/09/republicans-inflate-cost-of-taliban-seized-u-s-military-equipment/'}, {'title': 'FactChecking the Harris-Trump Debate', 'url': 'https://www.factcheck.org/2024/09/factchecking-the-harris-trump-debate/'}, {'title': 'How Many Americans and Allies Are Left in Afghanistan?', 'url': 'https://www.factcheck.org/2021/09/how-many-americans-and-allies-are-left-in-afghanistan/'}, {'title': 'Afghanistan Archives - FactCheck.org', 'url': 'https://www.factcheck.org/location/afghanistan/'}, {'title': \"Trump's False Military Equipment Claim - FactCheck.org\", 'url': 'https://www.factcheck.org/2020/07/trumps-false-military-equipment-claim/'}, {'title': '', 'url': None}]\n",
      "News Articles: [{'title': 'Republicans Inflate Cost of Taliban-Seized U.S. Military Equipment', 'url': 'https://www.factcheck.org/2021/09/republicans-inflate-cost-of-taliban-seized-u-s-military-equipment/'}, {'title': 'FactChecking the Harris-Trump Debate', 'url': 'https://www.factcheck.org/2024/09/factchecking-the-harris-trump-debate/'}, {'title': 'How Many Americans and Allies Are Left in Afghanistan?', 'url': 'https://www.factcheck.org/2021/09/how-many-americans-and-allies-are-left-in-afghanistan/'}, {'title': 'Afghanistan Archives - FactCheck.org', 'url': 'https://www.factcheck.org/location/afghanistan/'}, {'title': \"Trump's False Military Equipment Claim - FactCheck.org\", 'url': 'https://www.factcheck.org/2020/07/trumps-false-military-equipment-claim/'}, {'title': '', 'url': None}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4529/4215792878.py:111: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
      "/tmp/ipykernel_4529/4215792878.py:113: LangChainDeprecationWarning: The method `Chain.arun` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~ainvoke` instead.\n",
      "  response = await chain.arun({\"claim\": claim})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response displayed to the user:\n",
      "Based on the provided context, I have analyzed the claim:\n",
      "\n",
      "**Claim:** \"We would not have left $85 billion worth of brand-new, beautiful military equipment behind.\"\n",
      "\n",
      "**Classification:** Mostly False\n",
      "\n",
      "**Context and Background:**\n",
      "The claim seems to refer to a statement made by President Joe Biden or his administration during their debate with former President Donald Trump. The context suggests that the claim is related to the withdrawal of US forces from Afghanistan in 2021.\n",
      "\n",
      "**Key Facts and Evidence:**\n",
      "\n",
      "* According to various sources, including FactCheck.org, the US left behind around $7-8 billion worth of military equipment in Afghanistan, not $85 billion.\n",
      "* The equipment included vehicles, fuel, ammunition, and other supplies that were not recovered by the Taliban or US forces.\n",
      "* The equipment was likely left behind due to a combination of factors, including logistical challenges, security concerns, and the rapid pace of withdrawal.\n",
      "\n",
      "**Opposing Views and Evidence:**\n",
      "Some sources may report slightly different numbers or provide additional context. However, there is no evidence to support the claim that $85 billion worth of brand-new military equipment was left behind.\n",
      "\n",
      "**Correct Information:**\n",
      "It is estimated that around $7-8 billion worth of US military equipment was left behind in Afghanistan during the withdrawal. This amount does not include other supplies and resources that were also abandoned.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m handle_feedback(result, claim, context)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nest_asyncio.py:120\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[0;32m--> 120\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:315\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nest_asyncio.py:196\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    194\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[5], line 145\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m generate_context_and_assess_claim(claim, context)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m handle_feedback(result, claim, context)\n",
      "Cell \u001b[0;32mIn[5], line 122\u001b[0m, in \u001b[0;36mhandle_feedback\u001b[0;34m(response, claim, context)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mResponse displayed to the user:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[0;32m--> 122\u001b[0m rating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlease rate the response on a scale of [good, bad, mostly relevant, mostly not relevant]: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    124\u001b[0m feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo you have more context or corrections to provide? (y/n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feedback \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "async def fetch_article_content(url):\n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching article content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "async def factcheck_parser(claim):\n",
    "    claim = claim.replace(\" \", \"%20\")\n",
    "    url = f\"https://www.factcheck.org/search/#gsc.tab=0&gsc.q={claim}&gsc.sort=\"\n",
    "    print(url)\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)  # Launch browser in headless mode\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.wait_for_timeout(5000)  # Wait for the page to load\n",
    "\n",
    "        articles = await page.query_selector_all('div.gs-webResult.gs-result')\n",
    "        results = []\n",
    "\n",
    "        for article in articles:\n",
    "            title_tag = await article.query_selector('a.gs-title')\n",
    "            if title_tag:\n",
    "                title = await title_tag.inner_text()\n",
    "                link = await title_tag.get_attribute('href')\n",
    "                results.append({'title': title.strip(), 'url': link})\n",
    "\n",
    "        await browser.close()\n",
    "        return results\n",
    "\n",
    "async def retrieve_articles(claim):\n",
    "    articles = []\n",
    "    articles.extend(await factcheck_parser(claim))\n",
    "    return articles\n",
    "\n",
    "# work in prograss: If no articles were found in any of the three datasets, then use\n",
    "# newsAPI to try to find articles about the claim\n",
    "def fetch_news_articles(claim):\n",
    "    url = (\n",
    "        'http://newsapi.org/v2/everything?'\n",
    "        f'q={claim}&'\n",
    "        'language=en&'\n",
    "        'sortBy=relevancy&'\n",
    "        'pageSize=30&'\n",
    "        'apiKey=4ac92a95346643fdbdb26a7e4d0e98b1'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Throw an error for bad responses\n",
    "        news_data = response.json()\n",
    "        return news_data.get('articles', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news articles: {e}\")\n",
    "        return []\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "\n",
    "async def generate_context_and_assess_claim(claim, context):\n",
    "    # Fetch news articles related to the claim\n",
    "    if context == \"\":\n",
    "      news_articles = await retrieve_articles(claim)\n",
    "      print(\"FactCheck Articles:\", news_articles)\n",
    "\n",
    "      # Combine articles from sources. still to do\n",
    "      news_articles += fetch_news_articles(claim)\n",
    "      print(\"News Articles:\", news_articles)\n",
    "\n",
    "      if news_articles:\n",
    "          # Limit to top 3 articles\n",
    "          top_articles = news_articles[:3]\n",
    "\n",
    "          context += \" Here are some recent summaries that provide context:\\n\"\n",
    "          for article in top_articles:\n",
    "              title = article['title']\n",
    "              url = article['url']\n",
    "\n",
    "              # Get the article content\n",
    "              full_content = await fetch_article_content(url)\n",
    "              description = article.get('description')\n",
    "\n",
    "              if not description:\n",
    "                  if full_content:\n",
    "                      soup = BeautifulSoup(full_content, 'html.parser')\n",
    "                      first_paragraph = soup.find('p')\n",
    "                      description = first_paragraph.get_text(strip=True) if first_paragraph else 'No content available.'  \n",
    "\n",
    "                  else:\n",
    "                      description = 'No content available.'\n",
    "\n",
    "              article_context = f\"- **{title}**: {description[:200]}... [Read more]({url})\\n\"  # Truncate to 500 chars\n",
    "              context += article_context\n",
    "    \n",
    "    context_template = f\"\"\"\n",
    "    You are an assistant that provides factual information.\n",
    "    Analyze the following claim: '{claim}'.\n",
    "    Context: {context}\n",
    "    1. State if it is true, false, mostly true, or mostly false.\n",
    "    2. Provide relevant context or background information.\n",
    "    3. List key facts and evidence related to this claim.\n",
    "    4. Mention opposing views or evidence.\n",
    "    5. If the claim is false, provide the correct information.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=context_template, input_variables=[\"claim\"])\n",
    "    chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
    "\n",
    "    response = await chain.arun({\"claim\": claim})\n",
    "\n",
    "    return response\n",
    "\n",
    "async def handle_feedback(response, claim, context):\n",
    "  while True:\n",
    "      print(\"\\nResponse displayed to the user:\")\n",
    "      print(response)\n",
    "\n",
    "      rating = input(\"Please rate the response on a scale of [good, bad, mostly relevant, mostly not relevant]: \").lower()\n",
    "\n",
    "      feedback = input(\"Do you have more context or corrections to provide? (y/n): \").lower()\n",
    "      \n",
    "      if feedback == 'y':\n",
    "          additional_context = input(\"Please provide your additional context or corrections: \")\n",
    "          context += f\"\\nAdditional Context: {additional_context}\"\n",
    "      else:\n",
    "        return\n",
    "\n",
    "      if rating in ['bad', 'mostly not relevant']:\n",
    "          print(\"\\nRe-running the LLM chain with updated context...\")\n",
    "          new_response = await generate_context_and_assess_claim(claim, context)\n",
    "          print(\"\\nUpdated Response based on your feedback:\")\n",
    "          print(new_response)\n",
    "      else:\n",
    "          print(\"Thank you for your feedback! No need for further changes.\")\n",
    "\n",
    "async def main():\n",
    "    claim = \"We would not have left $85 billion worth of brand-new, beautiful military equipment behind\"\n",
    "    claim = input(\"Please enter a claim to be fact-checked: \")\n",
    "    context = \"\"\n",
    "    result = await generate_context_and_assess_claim(claim, context)\n",
    "    await handle_feedback(result, claim, context)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
