This code takes in a claim as a string an first parses it into multiple sentences then for each sentence it asynctronously calls into three datasets to check if the claims are true or false. Then it returns a result

TODO:
1. Give other ratings besides true or false for a claim(s)
2. return a link to an article(s) with the true statement(s) // Sally
3. integrate user feedbck
https://github.com/ParthaPRay/Ollama_GoogleColab_colabxterm_langchain/blob/main/Ollama_on_Colab_using_xterm_and_langchain.ipynb


https://stackoverflow.com/questions/8551230/how-can-i-get-information-from-an-a-href-tag-within-div-tags-with-beautifuls

https://www.mirascope.com/post/langchain-prompt-template


!pip install colab-xterm -q
%load_ext colabxterm
%xterm


% In xtrem run the following lines

curl -fsSL https://ollama.com/install.sh | sh

ollama serve

ollama pull llama2
%xterm

!pip install langchain -q
!pip install langchain-core -q
!pip install langchain-community -q
!pip install colab-xterm -q
!pip install fuzzywuzzy -q

from fuzzywuzzy import fuzz
from fuzzywuzzy import process
import nest_asyncio
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt')
from langchain.prompts import PromptTemplate
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import Ollama




import requests
import asyncio
import aiohttp
import nest_asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import Ollama
!pip install nest_asyncio


nest_asyncio.apply()

ollama_model = Ollama(model='llama2')

async def fetch_article_content(url):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                response.raise_for_status()
                return await response.text()
    except Exception as e:
        print(f"Error fetching article content from {url}: {e}")
        return ""

async def factcheck_parser(claim):
    claim = claim.replace(" ", "%20")
    url = f"https://www.factcheck.org/search/#gsc.tab=0&gsc.q={claim}&gsc.sort="

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # Launch browser in headless mode
        context = await browser.new_context()
        page = await context.new_page()
        await page.goto(url)
        await page.wait_for_timeout(5000)  # Wait for the page to load

        articles = await page.query_selector_all('div.gs-webResult.gs-result')
        results = []

        for article in articles:
            title_tag = await article.query_selector('a.gs-title')
            if title_tag:
                title = await title_tag.inner_text()
                link = await title_tag.get_attribute('href')
                results.append({'title': title.strip(), 'url': link})

        await browser.close()
        return results

async def retrieve_articles(claim):
    articles = []
    articles.extend(await factcheck_parser(claim))
    return articles

def fetch_news_articles(claim):
    url = (
        'http://newsapi.org/v2/everything?'
        f'q={claim}&'
        'language=en&'
        'sortBy=relevancy&'
        'pageSize=30&'
        'apiKey=4ac92a95346643fdbdb26a7e4d0e98b1'
    )

    try:
        response = requests.get(url)
        response.raise_for_status()  # Throw an error for bad responses
        news_data = response.json()
        return news_data.get('articles', [])
    except Exception as e:
        print(f"Error fetching news articles: {e}")
        return []

async def generate_context_and_assess_claim(claim):
    # Fetch news articles related to the claim
    news_articles = await retrieve_articles(claim)
    print("FactCheck Articles:", news_articles)

    news_articles += fetch_news_articles(claim)  # Combine articles from sources still to do.
    print("News Articles:", news_articles)

    context = "Recent discussions suggest that this claim might relate to broader debates."
    if news_articles:
        # Limit to top 3 articles
        top_articles = news_articles[:3]

        context += " Here are some recent summaries that provide context:\n"
        for article in top_articles:
            title = article['title']
            url = article['url']

            # Attempt to fetch the article content to get the first paragraph
            full_content = await fetch_article_content(url)
            description = article.get('description')

            if not description:
                if full_content:
                    soup = BeautifulSoup(full_content, 'html.parser')
                    first_paragraph = soup.find('p')
                    description = first_paragraph.get_text(strip=True) if first_paragraph else 'No content available.'
                else:
                    description = 'No content available.'
            article_context = f"- **{title}**: {description} [Read more]({url})\n"
            context += article_context

    context_template = f"""
    You are an assistant that provides factual information.
    Analyze the following claim: '{claim}'.
    Context: {context}
    1. State if it is true, false, mostly true, or mostly false.
    2. Provide relevant context or background information.
    3. List key facts and evidence related to this claim.
    4. Mention opposing views or evidence.
    5. If the claim is false, provide the correct information.
    """

    # Create a prompt with the input variable
    prompt = PromptTemplate(template=context_template, input_variables=["claim"])
    chain = LLMChain(llm=ollama_model, prompt=prompt)

    # Run the chain with the claim as input
    response = await chain.arun({"claim": claim})

    return response

async def main():
    claim = "We would not have left $85 billion worth of brand-new, beautiful military equipment behind"
    result = await generate_context_and_assess_claim(claim)

    print("Generated Response:", result)

# Run the example
if __name__ == "__main__":
    asyncio.run(main())
