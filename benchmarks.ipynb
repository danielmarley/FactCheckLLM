{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bae29b4a-e869-4993-a5c4-57784cefcd07",
   "metadata": {},
   "source": [
    "# Benchmarking Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a26c1af-3af1-4046-909d-46365edaabc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from factCheck import factCheckSingleClaim, factCheckSingleClaimNoContext\n",
    "\n",
    "OLLAMA_HOST = \"http://host.docker.internal:11434\" # for when running within docker image\n",
    "# OLLAMA_HOST = \"http://localhost:11434\"\n",
    " \n",
    "ollama3 = Ollama(model=\"llama3\", base_url=OLLAMA_HOST)\n",
    "ollama32 = Ollama(model=\"llama3.2\", base_url=OLLAMA_HOST)\n",
    "\n",
    "claimCount = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d4c2db-0191-40f6-9ac5-427cc7f838bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(predictions, y):\n",
    "    for idx in range(len(predictions)):\n",
    "        if y[idx] in predictions[idx]:\n",
    "            predictions[idx] = y[idx];\n",
    "        else:\n",
    "            predictions[idx] = predictions[idx][0];\n",
    "    \n",
    "    # Ensure predictions and dataset labels are aligned\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    f1 = f1_score(y, predictions, average='weighted')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84c9ea99-b1cb-42b0-881a-d001cbd0a1ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from benchmarking.AVERITEC.feverEval import get_fever_sample\n",
    "\n",
    "dfFever = get_fever_sample(claimCount, './benchmarking/AVERITEC/data_train.json', 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6831e1-840e-4c27-ae1b-dc3ab0787b67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAIM: Cyril Ramaphosa: South Africa has launched a Youth Employment Service, which places unemployed youth in paid internships in companies\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: : A Muslim man in India was killed by a mob after being accused of cow slaughter.\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: Deputy president William Ruto: The number of people on health insurance in Kenya increased from 3 million to 7 million in the period 2014 to 2019.\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: Americans for Prosperity: The ExIm Bank provided taxpayer-backed handouts to foreign companies such as Air China and Saudi Aramco.”\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: Andrew Mar: £22 billion was taken out of welfare spending in England between 2010 and 2015.\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: Cold water affects the internal walls of the stomach. It affects the large intestine and results in cancer\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: C Michael Pickens: Deaths from COVID-19 are nearing zero in the U.S. in early July 2020.\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: Glenn Grothman: There is a correlation between vitamin D deficiencies and higher COVID-19 mortality rates\n",
      "Cache miss. Fetching data from website.\n",
      "FactCheck Articles: []\n",
      "News Articles: []\n",
      "CLAIM: Sanskar TV: Slaked lime can cure over 70 diseases.\n",
      "Cache miss. Fetching data from website.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "start_time = time.time()\n",
    "\n",
    "feverMapping = {\n",
    "    \"true\": [\"Supported\"],\n",
    "    \"mostly true\": [\"Supported\", \"Conflicting Evidence/Cherrypicking\"],\n",
    "    \"mostly false\": [\"Refuted\", \"Conflicting Evidence/Cherrypicking\"],\n",
    "    \"false\": [\"Refuted\"],\n",
    "    \"not enough evidence\": [\"Not Enough Evidence\", \"Conflicting Evidence/Cherrypicking\"]\n",
    "}\n",
    "\n",
    "modelResults = [{'name': 'FactCheck3'}, {'name': 'FactCheck32'}, {'name': 'Llama3'}, {'name': 'Llama32'}]\n",
    "for modelIndex, model in enumerate([ollama3, ollama32]):\n",
    "    # FactCheckLLM check\n",
    "    origPred = []\n",
    "    feverPred = []\n",
    "    responses = []\n",
    "    for row in dfFever.iterrows(): \n",
    "        pred = await factCheckSingleClaim(row[1]['claim'], model)\n",
    "        origPred.append(pred['label'].lower())\n",
    "        pred['label'] = feverMapping[pred['label'].lower()]\n",
    "        feverPred.append(pred['label'])\n",
    "        responses.append(pred['reply'])\n",
    "        \n",
    "    modelResults[modelIndex]['dataset'] = 'FEVER';\n",
    "    modelResults[modelIndex]['original-prediction'] = origPred;\n",
    "    modelResults[modelIndex]['mapped-predictions'] = feverPred;\n",
    "    modelResults[modelIndex]['res'] = responses;\n",
    "    \n",
    "    acc, f1 = evaluate_model(feverPred, dfFever['label'].values)\n",
    "    modelResults[modelIndex]['acc'] = acc\n",
    "    modelResults[modelIndex]['f1'] = f1\n",
    "    \n",
    "    # base model check\n",
    "    origPred = []\n",
    "    feverPred = []\n",
    "    responses = []\n",
    "    for row in dfFever.iterrows(): \n",
    "        pred = await factCheckSingleClaimNoContext(row[1]['claim'], model)\n",
    "        origPred.append(pred['label'].lower())\n",
    "        pred['label'] = feverMapping[pred['label'].lower()]\n",
    "        feverPred.append(pred['label'])\n",
    "        responses.append(pred['reply'])\n",
    "        \n",
    "    modelResults[modelIndex+2]['dataset'] = 'FEVER';\n",
    "    modelResults[modelIndex+2]['original-prediction'] = origPred;\n",
    "    modelResults[modelIndex+2]['mapped-predictions'] = feverPred;\n",
    "    modelResults[modelIndex]['res'] = responses;\n",
    "    \n",
    "    acc, f1 = evaluate_model(feverPred, dfFever['label'].values)\n",
    "    modelResults[modelIndex+2]['acc'] = acc\n",
    "    modelResults[modelIndex+2]['f1'] = f1\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(f\"{int(minutes)} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e37273-d42d-46c8-9e3a-6375e527b055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Benchmark data\n",
    "models =  [item[\"name\"] for item in modelResults]\n",
    "acc = [item[\"acc\"] for item in modelResults]\n",
    "f1 = [item[\"f1\"] for item in modelResults]\n",
    "\n",
    "# Bar chart parameters\n",
    "x = np.arange(len(models))  # Position of bars\n",
    "width = 0.4  # Width of bars\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(x, acc, width, color=['blue', 'green', 'purple', 'orange'])\n",
    "\n",
    "# Add labels, title, and custom ticks\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Acc. Scores')\n",
    "ax.set_title('Comparison of Benchmark Accuracy')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylim(0, 1)  # Adjust y-axis range if needed\n",
    "ax.bar_label(bars, fmt='%.2f', padding=3)  # Add values on top of bars\n",
    "\n",
    "# Show the chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3939e2-c642-4322-95d2-52884e6afb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bar chart parameters\n",
    "x = np.arange(len(models))  # Position of bars\n",
    "width = 0.4  # Width of bars\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(x, f1, width, color=['blue', 'green', 'purple', 'orange'])\n",
    "\n",
    "# Add labels, title, and custom ticks\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('F1 Scores')\n",
    "ax.set_title('Comparison of Benchmark F1')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylim(0, 1)  # Adjust y-axis range if needed\n",
    "ax.bar_label(bars, fmt='%.2f', padding=3)  # Add values on top of bars\n",
    "\n",
    "# Show the chart\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e6889-743e-46f2-9f60-6d40f9e760b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
